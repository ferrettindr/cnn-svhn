\relax 
\newlabel{eq:sigmoid}{{1}{2}}
\newlabel{eq:softmax}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A feedforward network where on each neuron there is a label representing the value computed by it and passed to the nodes of the next layer through the arcs connecting them. A neuron assigns a weight $ \omega $ to each incoming arch and performs the shown linear combination. The sigmoid funciton of equation (1\hbox {}) can be used as the activation function $\sigma $. The function computed by the neurons in the output layer could be $softmax$ (2\hbox {}) where $\boldsymbol  {v}_i = \boldsymbol  {\omega }_{y,i}^T\boldsymbol  {n}_k + b_{y,i}$. Here the bias $ b $ is explicited in the computation rather than adding another dimension to the various vectors $ \boldsymbol  {\omega } $, $ \boldsymbol  {n}$, and $ \boldsymbol  {x} $ so that $ \omega _{r,i,0} = b_{r,i} $ for $ r \in \{j,k,y\} $ and $ x_{0} = n_{p,0} = 1 $ for $p \in \{j,k\} $.}}{3}}
\newlabel{fig:mlp}{{1}{3}}
\newlabel{eq:cross-entropy}{{3}{3}}
