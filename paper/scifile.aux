\relax 
\newlabel{eq:sigmoid}{{1}{2}}
\newlabel{eq:softmax}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A feedforward network where on each neuron there is a label representing the value computed by it and passed to the nodes of the next layer through the arcs connecting them. A neuron assigns a weight $ \omega $ to each incoming arch and performs the shown linear combination. The sigmoid funciton of equation (1\hbox {}) can be used as the activation function $\sigma $. The function computed by the neurons in the output layer could be $softmax$ (2\hbox {}) where $\boldsymbol  {v}_i = \boldsymbol  {\omega }_{y,i}^T\boldsymbol  {n}_k + b_{y,i}$. Here the bias $ b $ is explicited in the computation rather than adding another dimension to the various vectors $ \boldsymbol  {\omega } $, $ \boldsymbol  {n}$, and $ \boldsymbol  {x} $ so that $ \omega _{r,i,0} = b_{r,i} $ for $ r \in \{j,k,y\} $ and $ x_{0} = n_{p,0} = 1 $ for $p \in \{j,k\} $.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mlp}{{1}{3}}
\newlabel{eq:cross-entropy}{{3}{4}}
\newlabel{eq:sgd}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A visual representation of a convolutional neural network applied to image classification. The main components are: the convolutional layers producing the feature maps, the pooling layers used to downsample the convolutional layers output, and a standard fully connected feedforward network to handle classification\relax }}{7}}
\newlabel{fig:cnn}{{2}{7}}
\newlabel{fig:svhn1}{{3a}{8}}
\newlabel{sub@fig:svhn1}{{a}{8}}
\newlabel{fig:svhn2}{{3b}{8}}
\newlabel{sub@fig:svhn2}{{b}{8}}
\newlabel{fig:svhn}{{\caption@xref {fig:svhn}{ on input line 176}}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (a) The original images obtained from Street View. The blue rectangles describe the position of each digit in the images, they are shown for clarity but are not present in the images, their coordinates are stored in a separate file. (b) The images cropped and centered around a single digit. Note the presence of other digits to the sides of the one centered\relax }}{8}}
\newlabel{fig:hista}{{4a}{9}}
\newlabel{sub@fig:hista}{{a}{9}}
\newlabel{fig:histb}{{4b}{9}}
\newlabel{sub@fig:histb}{{b}{9}}
\newlabel{fig:histc}{{4c}{9}}
\newlabel{sub@fig:histc}{{c}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Histograms representing the absolute frequency of the labels for the training set (a), validation set (b), and test set (c). The distribution is very similar among the various sets, making it sound to use them for machine learning tasks.\relax }}{9}}
\newlabel{fig:hist}{{4}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces hyperparameters and their values at each phase of the training\relax }}{13}}
\newlabel{tab:def-hyper}{{1}{13}}
\newlabel{fig:valA}{{5a}{13}}
\newlabel{sub@fig:valA}{{a}{13}}
\newlabel{fig:valB}{{5b}{13}}
\newlabel{sub@fig:valB}{{b}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Loss and accuracy on the validation and reduced training set of the final enlarged model after each epoch of training on the reduced training set.\relax }}{13}}
\newlabel{fig:val_graphs}{{5}{13}}
\newlabel{fig:trainA}{{6a}{14}}
\newlabel{sub@fig:trainA}{{a}{14}}
\newlabel{fig:trainB}{{6b}{14}}
\newlabel{sub@fig:trainB}{{b}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Loss and accuracy on the test and training set of the final enlarged model after each epoch of training on the complete training set\relax }}{14}}
\newlabel{fig:train_graphs}{{6}{14}}
